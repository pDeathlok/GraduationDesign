

%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{times,amsmath,epsfig}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{subfigure}

\usepackage{url}
%\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
%\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
%\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|
%\newcommand{\keywords}[1]{\par\addvspace\baselineskip
%\noindent\keywordname\enspace\ignorespaces#1}


\begin{document}

\mainmatter  % start of an individual contribution

\pagestyle{empty}

% first the title is needed
\title{Mining Probabilistic High Utility Itemsets in Uncertain Databases}

% a short form should be given in case it is too long for the running head
%\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Yuqing Lan
%\thanks{Please note that the LNCS Editorial assumes that all authors have used
%the western naming convention, with given names preceding surnames. This determines
%the structure of the names in the running heads and the author index.}%
\and Yang Wang \and Shengwei Yi \and Dan Yu \and Simin Yu
}

%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published

\institute{School of Computer Science and Engineering, Beihang University, China\\
\email{\{lanyuqing, yangwang, yishengwei, yudan, siminyu\}@buaa.edu.cn}
}


%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}

Recently, with the growing popularity of Internet of Things (IoT) and pervasive computing, a large amount of uncertain data, i.e. RFID data, sensor data, real-time monitoring data, etc., has been collected. As one of the most fundamental issues of uncertain data mining, the problem of mining uncertain frequent itemsets has attracted much attention in the database and data mining communities. Although some efficient approaches of mining uncertain frequent itemsets have been proposed, most of them only consider each item in one transaction as a random variable following the binary distribution and ignore the unit values of items in the real scenarios. In this paper, we focus on the problem of mining probabilistic high utility itemsets in uncertain databases (MPHU), in which each item has a unit value. In order to solve the MPHU problem, we propose a novel mining framework, called UUIM, which not only includes an efficient mining algorithm but also contains an effective pruning technique. Extensive experiments on both real and synthetic datasets verify the effectiveness and efficiency of proposed solutions.





%\keywords{Correlated Uncertain Graphs, Shortest Path, Sampling}
\end{abstract}


\section{Introduction}
\label{sec:intro}

\vspace{-0.3cm}

Recently, with the growing popularity of Internet of Things (IoT) and pervasive computing, a large amount of uncertain data, i.e. RFID data, sensor data, real-time monitoring data, etc., has been collected. As one of the most fundamental issues of uncertain data mining, the problem of mining uncertain frequent itemsets has attracted much attention in the database and data mining communities. Although some efficient approaches of mining uncertain frequent itemsets have been proposed, most of them only consider each item in one transaction as a random variable following the binary distribution and ignore the unit values of items in the real scenarios.  In recommender system (e.g. music, video) analysis, mining frequent itemsets from an uncertain database refers to the discovery of itemsets which may frequently appear together in the records (transactions). However, the unit values (profits) of items are not considered in the framework of uncertain frequent itemsets mining. Hence, it cannot satisfy the requirement of a user who is interested in discovering itemsets with high enjoyment values. For example, consider the table of song data shown in Table~\ref{tab:song}. A and B in the table have different scores which depend on the evaluation of most users. The table of records is shown in Table~\ref{tab:record}. Jack listened A (Big Big World) before. The probable value that Jack enjoys Big Big World is 0.8. When songs are recommended, not only the frequency of songs in records of different user should be considered, but the popularity of songs should also be taken into account. In view of this, utility mining will emerge as an important topic in data mining for discovering itemsets with high utility, such as values (profits), in uncertain databases.

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{A Table of Songs}}
\vspace{-0.1cm}
  \label{tab:song}
  \begin{tabular}{|c|c|c|}
    \hline
    \bfseries {ID} & \bfseries {Song Name} & \bfseries {Score} \\
    \hline
    A & Big Big World & 3 \\
    \hline
    B & Someone Like You & 4 \\
    \hline
    C & Don¡¯t You Remember & 1 \\
    \hline
    D & My Love & 1 \\
    \hline
    E & Time to say goodbye & 8 \\
    \hline
    F & Right Here Waiting & 2 \\
    \hline
    G & Can You & 1 \\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-0.4cm}
\end{table}

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{A Table of Records}}
\vspace{-0.1cm}
  \label{tab:record}
  \begin{tabular}{|c|c|}
    \hline
    \bfseries {User} & \bfseries {Record and matching analysis} \\
    \hline
    Jack & (A, 0.8) (B, 0.8) (C, 0.7) (D, 0.7) \\
    \hline
    Rose & (A, 0.9) (C, 0.8) (G, 0.8) \\
    \hline
    Tom & (A, 0.6) (B, 0.7) (C, 0.6) (D, 0.5) (E, 0.8) (F, 0.5) \\
    \hline
    Peter & (B, 0.7) (C, 0.8) (D, 0.5) \\
    \hline
    Linda & (B, 0.8) (C, 0.7) (F, 0.4) \\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-0.4cm}
\end{table}

Mining high utility itemsets from the databases refers to finding the itemsets with high utilities. The basic meaning of utility is the interestingness / importance / profitability of an item to the users. Before finding high utility itemsets over uncertain databases, the definition of the high utility itemset is the most essential issue. In deterministic data, the utility of items in a transaction database consists of two aspects: (1) the importance of distinct items, which is called external utility, and (2) the importance of the items in the transaction, which is called internal utility. The utility of an itemset is defined as the external utility multiplied by the internal utility. An itemset is called a high utility itemset if its utility is no less than a user-specified threshold. However, different from the deterministic case, the utility of items in an uncertain transaction database only contain the importance of distinct items, which is called external utility in deterministic database. The definition of a high utility itemset over uncertain data has two different semantic explanations: expected support-based high utility itemset and probabilistic high utility itemset. However, the two definitions are different on using the random variable to define high utility itemsets. In the definition of the expected support-based high utility itemset, the expectation of the utility of an itemset is defined as the measurement, called as the expected utility of this itemset. In this definition, an itemset is of high utility if and only if the expected utility of such itemset is no less than a specified minimum expected utility threshold, min_util. In the definition of probabilistic utility itemset, the probability that the utility of an itemset is no less than the threshold is defined as the measurement, called as the high utility probability of an itemset, and an itemset is high utility if and only if the high utility probability of such itemset is larger than a given probabilistic threshold.

Mining high utility itemsets from uncertain databases is an important task which is essential to a wide range of applications such as recommender system analysis, Internet of Things (IoT) and biomedical applications. The definition of probabilistic high utility itemset includes the complete probability distribution of the utility of an itemset. Although the expectation is known as an important statistic, it cannot show the complete probability distribution. Hence, we mainly discuss mining probabilistic high utility itemsets in this paper.


\vspace{-0.5cm}
\subsubsection*{To sum up, we make the following contributions:}

\begin{itemize}
\vspace{-0.2cm}
\item	To the best of our knowledge, this is the first work to formulate the problem of mining probabilistic high utility itemsets in uncertain databases (MPHU).
\item   Due to the challenges from utility constraints, we propose a novel mining framework, called UUH-mine, which not only includes an efficient mining algorithm but also contains an effective pruning technique.
\item   We verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.
\vspace{-0.2cm}
\end{itemize}

The rest of the paper is organized as follows. Preliminaries and our problem formulation are introduced in Section 2. In Section 3, we present a novel mining framework, called UUH-mine. Based on this framework, an efficient mining algorithm and an effective pruning technique is devised in Section 4. In Section 5, experimental studies on both real and synthetic datasets are reported. In Section 6, we review the existing works. Finally, we conclude this paper in Section 7.

\section{Preliminaries and Problem Definitions}
\label{sec:model-ProState}

\vspace{-0.3cm}
In this section, we first introduce some basic concepts and then define the problem of mining probabilistic high utility itemsets in uncertain databases.

\vspace{-0.3cm}
\subsection{Preliminaries}
\label{sec:sec:model}

We first review the classical problem of frequent pattern mining.

Given a finite set of items I = \{$i_1,i_2$,...,$i_n$\}. An itemset X is a subset of items, i.e., X $\subseteq$ I. For the sake of brevity, an itemset X = \{$i_1,i_2$,...,$i_m$\} is also denoted as X = $i_1i_2$...$i_m$. A transaction T = (tid, Y) is a 2-tuple, where tid is a transaction-id and Y is an itemset. A transaction T = (tid, Y) is said to contain itemset X if and only if X $\subseteq$ Y. A transaction database D is a set of transactions. The number of transactions in D containing itemset X is called the support of X, denoted as sup(X). Given a transaction database D and a support threshold min\_sup, an itemset X is a frequent pattern, or a pattern in short, if and only if sup(X) $\geq$ min\_sup.

The problem of frequent pattern mining is to find the complete set of frequent patterns in a given transaction database with respect to a given support threshold.

In the problem of high utility pattern mining, each item $i_p$($1\leq p\leq n$) has a unit profit $p(i_p)$, and each item $i_p$ in the transaction $T_d$ is associated with a quantity q($i_p$,$T_d$), that is, the purchased number of $i_p$ in $T_d$. The utility of an item $i_p$ in the transaction $T_d$ is denoted as u($i_p$,$T_d$)and defined as $p(i_p)\times q(i_p, T_d)$. The utility of an itemset $X$ in $T_d$ is denoted as $u(X,T_d)$ and defined as $\sum_{i_p\in X\wedge X\subseteq T_d}u(i_p,T_d)$. The utility of an itemset $X$ in $D$ is denoted as $u(X)$ and defined as $\sum_{X\subseteq T_d\wedge T_d\in D}u(X,T_d)$.An itemset is called a high utility itemset if its utility is no less than a user-specified minimum utility threshold which is denoted as $min\_util$. Otherwise, it is called a low utility itemset.

Given a transaction database D and a user-specified minimum utility threshold $min\_util$, mining high utility itemsets from the transaction database is equivalent to discovering from D all itemsets whose utilities are no less than $min\_util$.

All the above mentioned problems are based on the deterministic databases. When mining high utility itemsets in uncertain databases, there will be lots of differences.

\vspace{-0.9cm}
\subsection{Problem Definitions}
\label{sec:sec:pro-state}
\vspace{-0.2cm}

In this subsection, we give several basic definitions about mining high utility itemsets over uncertain databases.

Let $I =\{i_1,i_2,...,i_n\}$ be a set of distinct items. Each item $i_p$ has a unit value $v(i_p)$. We name a non-empty subset, X, of I as an itemset. For brevity, we use $X = x_1x_2...x_n$ to denote itemset $X = \{x_1,x_2,...x_n\}$. X is a l - itemset if it has l items. Given an uncertain transaction database UD, each transaction is denoted as a tuple $< tid, Y >$ where $tid$ is the transaction identifier, and $Y = \{y_1(p_1),y_2(p_2),...,y_n(p_n)\}$. Y contains m units. Each unit has an item $y_i$ and probability, $p_i$, denoting the possibility of item $y_i$ appearing in the $tid$ tuple. The number of transactions containing $X$ in $UD$ is a random variable, denoted as $sup(X)$. Given $UD$, the expected support-based high utility itemsets and probabilistic high utility itemsets are defined as follows.

$Definition 1$. (Expexted Support) Given an uncertain transaction database $UD$ which includes $N$ transactions, and an itemset $X$, the expected support of $X$ is:

$esup(X)=\sum_{i = 1}^{N}p_i(X)$

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{An Uncertain Database}}
\vspace{-0.1cm}
  \label{tab:ud}
  \begin{tabular}{|c|c|}
    \hline
    \bfseries {TID} & \bfseries {Transaction} \\
    \hline
    $T_1$ & (A, 0.8) (B, 0.8) (C, 0.7) (D, 0.7) \\
    \hline
    $T_2$ & (A, 0.9) (C, 0.8) (G, 0.8) \\
    \hline
    $T_3$ & (A, 0.6) (B, 0.7) (C, 0.6) (D, 0.5) (E, 0.8) (F, 0.5) \\
    \hline
    $T_4$ & (B, 0.7) (C, 0.8) (D, 0.5) \\
    \hline
    $T_5$ & (B, 0.8) (C, 0.7) (F, 0.4) \\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-1.0cm}
\end{table}

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{Value Table}}
\vspace{-0.1cm}
  \label{tab:vt}
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \bfseries {Item} & \bfseries {A} & \bfseries {B} & \bfseries {C} & \bfseries {D} & \bfseries {E} & \bfseries {F} & \bfseries {G}\\
    \hline
    Value & 3 & 4 & 1 & 1 & 8 & 2 & 1\\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-0.5cm}
\end{table}

Different from deterministic databases, the utility of an itemset $X$ in $T_d$ is denoted as $u(X,T_d)$ and defined as $\sum_{i_p\in X}v(i_p)$ in uncertain databases.

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{The Probability Distribution of sup(A)}}
\vspace{-0.1cm}
  \label{tab:pd}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \bfseries {sup(A)} & \bfseries {0} & \bfseries {1} & \bfseries {2} & \bfseries {3}\\
    \hline
    Probability & 0.008 & 0.116 & 0.444 & 0.432\\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-0.4cm}
\end{table}

$Definition 2$. (Expected Utility) Given an uncertain transaction database $UD$ which includes $N$ transactions, and an itemset $X$, the expected utility of $X$ is:

$EU(X)= U(X,T_d)\times esup(X)$

$Definition 3$. (Expected Support-based High Utility Itemset) Given an uncertain transaction database $UD$ which includes $N$ transactions, and a minimum expected utility ratio, min\_eutil, an itemset X is an expected support-based high utility itemset if and only if $EU(X)\geq N \times min\_eutil$

$Definition 4$. (High Utility Probability) Given an uncertain transaction database $UD$ which includes $N$ transactions, a minimum utility ratio $min\_util$, and an itemset X, X's high utility probability, denoted as Pr(X), is shown as follows:

$Pr(X)= Pr\{sup(X)\times U(X,T_d)\geq N \times min\_util\}$

$Definition 5$. (Probabilistic High Utility Itemset) Given an uncertain transaction database $UD$ which includes $N$ transactions, a minimum utility ratio $min\_util$, and a probabilistic high utility threshold $put$, an itemset $X$ is a probabilistic high utility itemset if X's high utility probability is larger than the probabilistic high utility threshold, namely,

$Pr(X)= Pr\{sup(X)\times U(X,T_d)\geq N \times min\_util\}\geq put$

$Example$ (Probabilistic High Utility Itemset) Given an uncertain database in Table 5, $min\_util$ = 1 and probabilistic high utility threshold $put$ = 0.9, $sup(A)\geq min\_util \times N \div U(A,T_d)$. So, the high utility probability of A is: $Pr(A) = Pr\{sup(A)\geq 5 \times 1 \div 3\} = Pr\{sup(A)\geq 2\} = Pr\{sup(A) = 2\} + Pr\{sup(A) = 3\} = 0.444 + 0.432 = 0.876 < 0.9 = put$. Thus, {A} is not a probabilistic high utility itemset.

We are now able to specify the Mining Probabilistic High Utility Itemsets (MPHU) problem as follows: Given an uncertain transaction database UD, a minimum utility threshold $min\_util$ and a probabilistic high utility threshold $put$, the problem of MPHU is to find all probabilistic high utility itemsets in uncertain databases.

\vspace{-0.3cm}
\section{UUH-mine Framework}
\label{sec:base}
\vspace{-0.3cm}

In order to solve the MPHU problem, we propose a novel mining framework called UUH-Mine which is based on the divide-and-conquer framework and the depth-first search strategy. The algorithm was extended from the H-Mine algorithm which is classical algorithm in deterministic frequent itemset mining.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{1.eps}\\
  \caption{UUH-Struct Generated from Table 3}
  \label{fig1}
\end{figure}

UUH-Mine algorithm can be outlined as follows. Firstly, it scans the uncertain database and finds all items. Then, the algorithm builds a head table which contains all items. For each item, the head table stores four elements: the label of this item, the value of this item, the expected support of such item, and a pointer domain. After building the head table, the algorithm inserts all transactions into the data structure, UUH-Struct.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{2.eps}\\
  \caption{UUH-Struct of Head Table of A}
  \label{fig2}
\end{figure}


In this data structure, each item is assigned with its label, its value, its appearing probability and a pointer. The UUH-Struct of Table 3 and Table 4 is shown in Figure 1. After building the global UUH-Struct, the algorithm uses the depth-first strategy to build the head table in Figure 2 where A is the prefix. Then, the algorithm recursively builds the head tables where different itemsets are prefix and generates all the itemsets.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{3.eps}\\
  \caption{UUH-struct after removing item A}
  \label{fig3}
\end{figure}

We use the UUH-Mine framework to find all high utility itemsets from the database through depth first search. However, this UUH-Mine framework needs to traverse all $2^n$ itemsets to find the result, so we must optimize it.

\vspace{-0.4cm}
\section{Optimization Strategies and Algorithms}
\label{sec:impro-samp}
\vspace{-0.2cm}

In this section, we first introduce some optimization Strategies to improve the UUH-Mine framework. Then, we illustrate our UUIM algorithm, which is called Uncertain Utility Itemsets Mining algorithm.

\vspace{-0.4cm}
\subsection{Optimization Strategies}
\label{sec:sec:unequl-pro-samp}

\vspace{-0.2cm}
$Definition 6$. For the given uncertain transaction T, its transaction maximum expected utility equals to the max expected utility of itemsets it contains, denoted as $MU(T)$:

$MU(T) = max\{EU(X)|X \subset T\}$

For example, the maximum expected utility of transaction 1 is 4.48, and the maximum expected utility of all 16 itemsets contained by transaction 1 is {A, B}, refer to Table 4.

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{Uncertain Database with Maximum Transaction Expected Utility}}
\vspace{-0.1cm}
  \label{tab:mt}
  \begin{tabular}{|c|c|c|}
    \hline
    \bfseries {TID} & \bfseries {Transaction} & \bfseries {MU}\\
    \hline
    $T_1$ & (A, 0.8) (B, 0.8) (C, 0.7) (D, 0.7) & 4.48 \\
    \hline
    $T_2$ & (A, 0.9) (C, 0.8) (G, 0.8) & 2.88 \\
    \hline
    $T_3$ & (A, 0.6) (B, 0.7) (C, 0.6) (D, 0.5) (E, 0.8) (F, 0.5) & 6.72 \\
    \hline
    $T_4$ & (B, 0.7) (C, 0.8) (D, 0.5) & 2.8 \\
    \hline
    $T_5$ & (B, 0.8) (C, 0.7) (F, 0.4) & 3.2 \\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-1.0cm}
\end{table}

In this example, the biggest transaction contains only 6 items, so we can calculate the maximum expected utility of every transaction through exhaustive method. However, in practical problems, a transaction may contains many items, so the exhaustive method is not efficient. Here we will introduce a fast way:

Given a transaction T whose length (the number of item it contains) is L, items in T is $\{i_1,i_2,...,i_L\}$, and the probability of each item is $\{p_1,p_2,...,p_L\}$. We can construct the sub problem $S_{X,j}$ (X represent an itemset), which means the maximum expected utility in set which contains itemsets derived from itemset X and the last j items in T. Obviously, we have EU(T) = $S_{\phi,L}$ and $S_{I,0}$ = EU(X,T) as well as recursive relation:

$S_{X,j} = max\{S_{{X\cup i_{L-j+1}},j-1},S_{X,j-1}\}$

We can get EU(T) from that recursive relation, but it still need to consider all $2^L$ itemsets. So we can optimize it by the following theorem:

$Theorem 1$. If there exist itemset $X_1$ and $X_2$ = $X_1 \cup \{i_j\}$ ($X_2$ represents a super-itemset which has one more item than $X_1$), and EU($X_1$,T) > EU($X_2$,T), then expected utility of $X_2$ and all super-itemsets of $X_2$ cannot be the maximum expected utility of T.

$Rationale 1$. The expected utility of $X_2$ obviously cannot be the maximum expected utility of T, so we will prove that it is also true for $X_2$'s super-itemsets $X_3$ = $X_2 \cup X'$. For one of $X_2$'s super itemsets, we can construct a new itemset $X_4$ = $X_1 \cup X'$, then we have:

$EU(X_3,T) = U(X_3) \times P(X_3,T)
= (U(X_2)+U(X'))\times P(X_2,T)\times P(X',T)
= (EU(X_2,T)+U(X'))\times P(X_2,T)\times P(X',T)$

$EU(X_4,T) = U(X_4) \times P(X_4,T)
= (U(X_1)+U(X'))\times P(X_1,T)\times P(X',T)
= (EU(X_1,T)+U(X'))\times P(X_1,T)\times P(X',T)$

Due to $EU(X_1,T) > EU(X_2,T)$ and $P(X_1,T)\geq P(X_2,T)$, we can know $EU(X_3,T) < EU(X_4,T)$, namely, the expected utility of $X_3$ cannot be the maximum expected utility of T.



\vspace{-0.2cm}
\[
S_{X,j}=\left\{ \begin{array}{ll}
S_{X,j} = max\{S_{{X\cup i_{L-j+1}},j-1},S_{X,j-1}\} & {(EU(X,T) < EU(X \cup \{i_{L-j+1}\},T))}\\
S_{X,j-1} & {(EU(X,T) \geq EU(X \cup \{i_{L-j+1}\},T))}
\end{array} \right.
\vspace{-0.2cm}
\]

Thus, we can greatly speed up the calculation of transaction maximum expected utility.

$Definition 7$. For itemset X and uncertain transaction database UD, the transaction maximum expected utility of itemset X is MU:

$MU(X) = \sum_{X\subseteq T\wedge T\in D} MU(T) $

which means the sum of the transaction maximum expected utilities of transactions containing itemset X.

We can derive Lemma 1 through definitions above:

$Lemma 1$. The transaction maximum expected utilities of X's super-itemsets are not more than the transaction maximum expected utilities of X. For $X \subseteq X'$, always $MU(X') \geq MU(X)$.

According to the Chernoff bound, suppose $X_1,X_2,...,X_n$ be independent random variables taking values in \{0,1\}. Let X denote their sum and let $\mu = E[X]$ denote the sum's expected value. For any $\delta > 0$ it holds that

$Pr(X > (1 + \delta)\mu)<(\frac{e^\delta}{(1+\delta)^(1+\delta)})^\mu$ ($\delta > 0$)

In the problem of mining probabilistic high utility itemsets, for one itemset X, its appearance in one uncertain transaction T can be seen as an independent Possion experiment, and the real support of X, i.e. the sup(X) is the sum of many Possion experiments, so the expect of that variable is expected suppprt count of X. The utility probability of X is:

$Pr(sup(X)\times U(X)\geq min\_util) = Pr(sup(X)\geq \frac{min\_util}{U(I)})$

When $esup(X) < min\_util / U(X)$, we can let $(1+ \delta) esup(X) = min\_util / U(X)$, so we can get

$\delta = \frac{min\_util}{U(X)esup(X)}-1 = \frac{min\_util}{EU(X)-1} $

$Pr(sup(X) \geq \frac{min\_util}{U(X)} )<(\frac{e^\delta}{(1+\delta)^(1+\delta)})^{esup(X)}$

While $\delta = min\_util / EU(X) - 1$. The right side of inequality is a decreasing function of $\delta$, so when $\delta$ decreases, the original inequality still holds. Hence we can get the following Lemma:

$Lemma 2$. For itemset X, given uncertain transaction database D, utility threshold min\_util and probabilistic utility threshold $put$ , if $MU(X) < min\_util$ and $(\frac{e^\delta}{(1+\delta)^(1+\delta)})^{esup(X)} < put$, then itemset X and all of its super-itemsets cannot be utility.

Lemma 2 is key point to solve the ¡°mining probabilistic high utility itemsets¡± problem in uncertain database, it can greatly reduce the search space so that the algorithm can be efficient.

We can use it to optimize the UUH-mine framework mentioned in last section, as shown in Fig.4. Because the transaction maximum expected utility of itemset G is 2.88 and cannot pass the check in Lemma 2, G and all of its¡¯ super itemsets cannot be high utility itemsets and we do not need to check them.

We can use the same method to optimize the other header tables generated by projection databases (such as $H_A$, $H_AB$, etc).


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.99\textwidth]{4.eps}\\
  \caption{UUH-mine data structure with optimization}
  \label{fig4}
\end{figure}

\vspace{-0.2cm}
\subsection{UUIM Algorithm}
\label{sec:sec:DijSampling}
\vspace{-0.1cm}

In this section, we will introduce UUIM (Uncertain Utility Itemsets Mining) algorithm in detail. First, we will give the overall outline of this algorithm, which divides the mining process into two phases, and then briefly describe them. Then we will explain the two phases in detail.

Algorithm 1(UUIM) is the algorithm framework of mining utility itemsets. This algorithm aims to mine all utility itemsets UIS from the given uncertain transaction database $UD$ through the given utility threshold $min\_util$ and probabilistic high utility threshold $put$. Line 1 is used to initialize the result set UIS; line 2 creates the initial header table H of the UUH-mine framework through function Initialize Header; line 3 uses the key function Recursion to search each items in depth first way; the last line return the calculation results which are all utility itemsets. According to this we can know that the main part of this algorithm consists of two parts, one for creating header table which is explained above in detail; the other is the recursive function Recursion which is used to traverse all probabilistic high utility itemsets.

Next, we will introduce how the key function Recursion works. In algorithm 2(Recursion), line 1 traverses each itemset in the header table; lines 2 and 3 check whether the new itemset is utility itemset. If it is true it will be added to result; line 4 checks whether that new itemset can pass the $Pr(sup(X) \geq \frac{min\_util}{U(X)} )<(\frac{e^\delta}{(1+\delta)^(1+\delta)})^{esup(X)}$. If yes, a header table will be created in line 5 and traversed in line 6.

In summary, this section explains the probabilistic utility itemset mining algorithm, UUIM. First we introduce a new data structure UUH-struct, which builds the foundation of the algorithm¡¯s efficiency; then we describe several algorithm optimization methods such as global bound and local pruning in order to speed up the algorithm.


\section{Performance Evaluations}
\label{sec:exp}
\vspace{-0.2cm}

In this section, we will report and analyze our experiment results. All the experiments are proceeded on a PC with CPU Inter(R) Core(TM)i7-2600, frequency 3.40GHz, memory 8.00GB, hard disk 500GB. The Operation System is Microsoft Windows 7 Enterprise Edition. The development software is Microsoft Visual Studio 2010, using language C++ and its standard template library (STL).

\vspace{-0.3cm}
\subsubsection{The Dataset}
\label{sec:exp:dataset}
\vspace{-0.1cm}

The real data used in this paper can be classified into two categories. One is sparse dataset such as Road Network data\footnote{http://www.cs.utah.edu/~lifeifei/SpatialDataset.htm}, and the other one is Social Network data\footnote{http://snap.stanford.edu/data}. The numbers of vertices and edges are listed in Table~\ref{tab:Rdataset}.

In real datasets, the edges are certain, so we need to change the certain graphs into uncertain graphs. In Road Network Datasets, we use the method which is introduced in~\cite{hua2010probabilistic}. Use Normal Distribution $N(\mu,\sigma)$ to generate the weights on each edge. Here, $\mu$ is the edge weight in original datasets, and $\sigma$ is the variance of the generated weights. $\sigma$ is different according to different edges, which is normally distributed as $N(\mu_\sigma,\sigma_\sigma)$. Here, $\mu_\sigma=xR$, and the value range of $x$ is $[1\%,5\%]$. In default condition, $\mu_\sigma=1\%R$, and $R$ is the value range of all weights in original datasets. In the same way, we generate the weights and probabilities on edges in Social Network datasets.

\begin{table}[htbp]
\vspace{-0.2cm}
  \centering
  \caption{\small{Real Dataset Parameters}}
\vspace{-0.1cm}
  \label{tab:Rdataset}
  \begin{tabular}{|c|c|c|}
    \hline
    \bfseries {Name of Dataset} & \bfseries {Vertex Number} & \bfseries {Edge Number} \\
    \hline
    OLdenburg (OL) Road Network & 6,105 & 7,035 \\
    \hline
    San Francisco Road Network (SF) & 174,956 & 223,001 \\
    \hline
    wiki-Vote & 7,115 & 103,689 \\
    \hline
    %soc-Epinions1 & 75,879 & 508,837 \\
%    \hline
    \end{tabular}
\vspace{-0.4cm}
\end{table}

For all the datasets, we choose 100 pairs of vertices as starting points and termination points. After the 100 tests, we calculate the average time cost, memory cost, \emph{Mean Square Error (MSE)} and relative error as the experiment results. We set the threshold defaulted to be 0.5 and compare the proformance of baseline sampling algorithm (denoted as \emph{BS}) and our improved \emph{DijSampling} algorithm (denoted as \emph{DS}).

\vspace{-0.2cm}

\subsubsection{Running Time}
\label{sec:exp:alg:time}

Fig.~\ref{fig:time} shows the running time vs sample size for the two sampling algorithms on different real datasets. From the results, we can observe that with the increase of sample size, the time cost for the two algorithms all increases. The time cost of \emph{BS} is always largest than that of \emph{DS}. In addition, we observe that the larger the graph is, the more time will be cost.

\begin{figure*}[htbp]
\vspace{-0.7cm}
  \centering
  \subfigure[\small{OL}]{
      \label{fig:time:ol}
      \includegraphics[width=0.32\textwidth]{fig-time-OL.eps}}
  \subfigure[\small{SF}]{
      \label{fig:time:sf}
      \includegraphics[width=0.32\textwidth]{fig-time-SF.eps}}
  \subfigure[\small{wiki-Vote}]{
      \label{fig:time:wiki}
      \includegraphics[width=0.32\textwidth]{fig-time-Wiki.eps}}
  %\subfigure[\small{soc-Epinions1}]{
%      \label{fig:time:soc}
%      \includegraphics[width=0.35\textwidth]{fig-time-Soc.eps}}
\vspace{-0.3cm}
  \caption{Running time vs Sample Size}
  \label{fig:time}
\vspace{-0.6cm}
\end{figure*}

The results above are reasonable. The running time depends on the number of sampled edges. The more edges sampled, the more time an algorithm will cost. As the \emph{BS} Algorithm samples more edges, the whole possible graph, its running time is longer.


\subsubsection{Memory Cost}
\label{sec:exp:memory}

\begin{figure*}[htbp]
\vspace{-1.1cm}
  \centering
  \subfigure[\small{OL}]{
      \label{fig:memory:ol}
      \includegraphics[width=0.32\textwidth]{fig-mem-OL.eps}}
  \subfigure[\small{SF}]{
      \label{fig:memory:sf}
      \includegraphics[width=0.32\textwidth]{fig-mem-SF.eps}}
  \subfigure[\small{wiki-Vote}]{
      \label{fig:memory:wiki}
      \includegraphics[width=0.32\textwidth]{fig-mem-Wiki.eps}}
  %\subfigure[\small{soc-Epinions1}]{
%      \label{fig:memory:soc}
%      \includegraphics[width=0.4\textwidth]{fig-mem-Soc.eps}}
\vspace{-0.3cm}
  \caption{Memory Cost vs Sample Size}
  \label{fig:memory}
\vspace{-0.6cm}
\end{figure*}

Fig.~\ref{fig:memory} shows the memory cost vs sample size on different datasets. It can be seen that with the increase of sample times, the memory cost of two algorithms increases. The memory cost of the two algorithms is almost the same.

The phenomena above is reasonable that as sample size increases, the program needs to explore more space to find the shortest path and calculate estimator with corresponding variance. Thus, the memory cost increases with the increase of sample times. Moreover, both the algorithms need to save the structures of graph data and some queues for \emph{Dijkstra} algorithm, which are the same. The only difference between the two methods are the flags showing whether each edge is sampled, which takes little memory cost. Thus, the memory cost of the two algorithms is nearly the same.

\vspace{-0.2cm}
\subsubsection{Accuracy}
\label{sec:exp:estim:var}

We test \emph{Mean Square Error (MSE)} and relative error of the estimators to show the accuracy of different algorithms. As the bias caused by formula~(\ref{eq:margin}) is always 0 in the experiment result, we do not show it in our result figures. Since the variance of \emph{BS} estimator is the same as the \emph{H-H} estimator of \emph{DS}, we show them using the same line in result figure. The method of calculating \emph{relative error} is the same as that in~\cite{jin2011distance}.

\begin{figure*}[htbp]
\vspace{-0.7cm}
  \centering
  \subfigure[\small{OL}]{
      \label{fig:mse:ol}
      \includegraphics[width=0.32\textwidth]{fig-var-OL.eps}}
  \subfigure[\small{SF}]{
      \label{fig:mse:sf}
      \includegraphics[width=0.32\textwidth]{fig-var-SF.eps}}
  \subfigure[\small{wiki-Vote}]{
      \label{fig:mse:wiki}
      \includegraphics[width=0.32\textwidth]{fig-var-Wiki.eps}}
  %\subfigure[\small{soc-Epinions1}]{
%      \label{fig:mse:soc}
%      \includegraphics[width=0.4\textwidth]{fig-var-Soc.eps}}
\vspace{-0.3cm}
  \caption{Mean Square Error vs Sample Size}
  \label{fig:mse}
\vspace{-0.6cm}
\end{figure*}

From Fig.~\ref{fig:mse}, it can be seen that no matter how the size of datasets change, the variance of estimators decreases as sample size increases. Moreover, the variance of $\widehat{SPr_{H-T}}$ always keeps smaller than that of $\widehat{SPr_{H-H}}$. This result means the $\widehat{SPr_{H-T}}$ estimator is better than $\widehat{SPr_{H-T}}$, which verifies the discussion in Section~\ref{sec:sec:unequl-pro-samp}.

\begin{figure*}[htbp]
\vspace{-0.7cm}
  \centering
  \subfigure[\small{OL}]{
      \label{fig:error:ol}
      \includegraphics[width=0.32\textwidth]{fig-err-OL.eps}}
  \subfigure[\small{SF}]{
      \label{fig:error:sf}
      \includegraphics[width=0.32\textwidth]{fig-err-SF.eps}}
  \subfigure[\small{wiki-Vote}]{
      \label{fig:error:wiki}
      \includegraphics[width=0.32\textwidth]{fig-err-Wiki.eps}}
  %\subfigure[\small{soc-Epinions1}]{
%      \label{fig:error:soc}
%      \includegraphics[width=0.4\textwidth]{fig-err-Soc.eps}}
\vspace{-0.3cm}
  \caption{Relative Error vs Sample Size}
  \label{fig:error}
\vspace{-0.6cm}
\end{figure*}

From Fig.~\ref{fig:error}, the relative error of \emph{BS} is always the largest, and \emph{H-T} of \emph{DS} is always the smallest. This means the stability and accuracy of $\widehat{SPr_{H-T}}$ is strongest. This result verifies our analysis in Section~\ref{sec:sec:DijSampling}. That is, we need fewer sampling times by applying \emph{DijSampling} Algorithm to get the same accuracy as baseline algorithm.

As we cannot get the distribution of unequal probability sampling, the error cannot be bounded. However, from Figure~\ref{fig:error}, in the 4 datasets used in our experiments, the error of both $\widehat{SPr_{H-T}}$ and $\widehat{SPr_{H-H}}$ is very low. Moreover, with the increase of sample times, the error fluctuates very gently. Thus, the estimators can approximate the exact answer well.


\vspace{-0.4cm}
\section{Related Works}
\label{sec:relate}
\vspace{-0.2cm}

The most popular shortest path algorithms over deterministic graphs are \emph{Dijkstra} \cite{bast2006transit} and $A^{*}$~\cite{fu2006heuristic}. These algorithms has a large time complexity. Thus, there are large number of works focusing on building indexes to speed up the algorithms. To accelerate the exact shortest path query, Cohen et al.~\cite{cohen2003reachability} proposed a 2-hop labeling method, and F.Wei~\cite{wei2010tedi} proposed a tree-width decomposition index. Moreover, authors in~\cite{gubichev2010fast} proposed a landmark encoding method to provide an approximate answer for shortest path query. In addition, there are also some shortest path algorithms over deterministic graphs designed typically for road networks such as~\cite{sankaranarayanan2009path}\cite{samet2008scalable}\cite{rice2010graph}\cite{jing1998hierarchical}, and \cite{wu2012shortest} is a good summary. In recent years, some shortest path algorithms are also designed to for large graph environment, such as \cite{jin2012highway}\cite{cheng2012efficient}\cite{gao2011relational}.

Essentially, the models of these algorithms above are all different from ours, so it is impossible to extend these algorithms directly.

Different queries are addressed in uncertain environment for a long time such as~\cite{tong2012mining} \cite{tong2012discovering}\cite{yuan2011efficient}\cite{yuan2013efficient}. Among them, shortest path query over uncertain graph is important. This query is first proposed by Loui~\cite{loui1983optimal}. Many works such as~\cite{yuan2010efficiently}\cite{zou2011top} considered an independent model, which is argued in detail in Section~\ref{sec:intro}. Moreover, Ming Hua et.al~\cite{hua2010probabilistic} built a simple correlated model. It modeled simple correlation in uncertain graphs between each two edges sharing the same vertex. But when there are more than two edges sharing the same vertex, it would be confusing.

In addition, ruoming Jin et.al~\cite{jin2011distance} applied \emph{Unequal Probability Sampling} method, which solved the uncertain reachability problem efficiently and effectively. However, their algorithms cannot be applied into our problem directly for three reasons. First, their model was independent uncertain graph model, and their algorithms cannot handle the correlated uncertain graph model. Secondly, their query was reachability, which is different from our shortest path query. As discussed in Section~\ref{sec:sec:pro-state}, our problem is harder than theirs. Thirdly, they applied their sampling algorithm in a \emph{divided and conquer} framework. If this framework is extended to our problem directly, we cannot find out the shortest path unless the last edge is sampled. Then, their algorithms would lose the advantages.

\vspace{-0.3cm}
%
\section{Conclusion}
%
\label{sec:conclu}
\vspace{-0.2cm}

In this paper, we first propose a new uncertain graph model, which considers the hidden correlation among edges sharing the same vertex. As calculating the \emph{shortest path probability} is a \#P-hard problem, we use sampling methods to approximately compute it. We propose a baseline algorithm and an improved algorithm. Our improved algorithm is more efficient than the baseline algorithm with more accurate answers when sampling the same times. Moreover, we preform comprehensive experiments to verify the efficiency and accuracy of our algorithms.


\vspace{-0.3cm}
\subsubsection*{Acknowledgments.} Yurong Cheng, Ye Yuan and Guoren Wang were supported by the NSFC
(Grant No.61025007, 61328202, 61202087, and 61100024), National Basic Research Program of China (973, Grant No.2011CB302200-G), National High Technology Research and Development 863 Program of China (Grant No.2012AA011004), and the Fundamental Research Funds for the Central Universities (Grant No. N110404011). Baiyou qiao was supported by the NSFC (Grant No.61073063).

\bibliographystyle{splncs03}
\bibliography{References}

\end{document}
